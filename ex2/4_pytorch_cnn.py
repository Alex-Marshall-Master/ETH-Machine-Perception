# -*- coding: utf-8 -*-
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
"""4_pytorch_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VB5CUiuAXAmPrumO1QJTMdq2gUBgdIm9

# PyTorch Tutorial - CNNs

In this tutorial, you will learn how to build a convolutional neural network (CNN) for image classification. We will be working with the well-known MNIST dataset featuring hand-written single digits. The CNNs task is then to identify which digit is shown on a given image.

The tutorial is based on the official image classification [PyTorch example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It is meant to teach you the basic tools needed to implement a convolutional architecture in PyTorch and it additionally shows how to leverage TensorBoard for some visualizations.

In the following, we assume familiarity with the PyTorch tutorial presented in the previous exercise, i.e., you should be aware of PyTorch's core concepts.

This tutorial consists of:
  1. Introduction to CNNs
  2. The MNIST Data Set
  3. Building the Model
  4. TensorBoard
  5. Concluding Remarks and Exercises

## Introduction to CNNs
### The Convolution Operation
A convolution is a mathematical operation and represents an essential building block in image processing tasks. Depending on what filter you use to convolve an image with, you can use convolutions to blur an image (i.e., removing high-frequency noise) ...

<center><img src="https://imgur.com/44GJQA8.png" align="middle" hspace="20px" vspace="5px"></center>

... detect edges ...

<center><img src="https://imgur.com/KIfP80j.png" align="middle" hspace="20px" vspace="5px"></center>

... and [many other things](http://setosa.io/ev/image-kernels/). You can think of a convolution as sliding a window, also called _kernel_ or _filter_, over each pixel of an image and computing a dot product between the filter's values and the image's values that the filter is covering. This operation produces one output value for every location on the image over which we slide the filter. Usually, we go through every pixel in the image and position the filter such that its center pixel lies on the image pixel. Hence, for pixels lying on the boundary of the image, we have to pad the image as the filter otherwise "spills over" (more on this later). A visualization of the convolution process looks like this (this and subsequent animations taken from [here](https://github.com/vdumoulin/conv_arithmetic)):

<center><img src="https://i.imgur.com/pTNYQE7.gif" align="middle" hspace="20px" vspace="5px"></center>

Here, blue is the input image, grey is the 3-by-3 filter that we are convolving the input image with and green is the output image. The dashed pixels represent padded regions.

### Convolutions in Neural Networks
In the context of CNNs, we are using exactly the same convolution operation, but we think of it in a slightly different way: Instead of producing a desired output image like in traditional image processing tasks (e.g. blurry, edges highlighted, etc.), a filter extracts certain _features_ from a local neighborhood and we store them in _feature maps_, sometimes also called _activation maps_ or simply _channels_. Moreover, we are moving several filters over a given image, so each convolutional layer potentially outputs several of those feature maps. Importantly, the weights of the filter are not fixed - those values are actually what the network must optimize. In other words, the network learns to set up those filters in such a way that they extract features from the images that are most useful for the network to solve the task at hand.

The following animation taken from the lecture slides summarizes all this:

<img src="https://imgur.com/JpUiKVQ.gif" align="middle" hspace="20px" vspace="5px">

### Types of Convolutions
When adding a convolutional layer, we must decide upon the following:
  - **Filter Size**: We have to decide how big a filter is, i.e. determine its width and height. Common choices are  small, square, and odd, e.g. 3-by-3, 5-by-5, 7-by-7, etc. Of course, this depends on the problem you are trying to solve. Increasing the filter sizes increases the amount of trainable parameters. CNNs are every effective models thanks to **weight-sharing** and **repetition of convolution operation**, which also provides invariance to certain image operations such as translation and rotation. The parameter space can be reduced considerably in comparison to fully connected layers. Choosing huge filter sizes goes against this intuition, and in recent research it was shown, that instead of increasing filter sizes, creating deeper models is generally a better idea.
  - **Number of Feature Maps**: We should also decide how many feature maps each layer outputs. Again, this design choice depends on the problem.
  - **Padding**: As mentioned above, when we apply the filter on the boundary of the image, the filter "spills over". Hence, we must decide what to do in these cases. PyTorch convolution layers have a tuple of integer *padding* attribute which is the amount of zero-padding added to both sides of the input. 
  - **Strides**: So far we always assumed that once we computed the output of a filter at a given location, we just move on the pixel right next to it. We could however also choose to omit some pixels inbetween. E.g., if we were to compute the convolution only on every other pixel, we would say that we use a stride of 2. This effectively reduces the size of the output image. Sometimes strided convolutions are used instead of pooling layers. The following example shows a convolution with the stride set to 2 on both the vertical and the horizontal axis of the image.
  <center><img src="https://imgur.com/kjgc33A.gif" align="middle" hspace="20px" vspace="5px"></center>
  
  - **Dilations**: In dilated convolutions, sometimes also called "Ã  trous", we introduce holes in the filter, i.e. we spread the filter over a wider area but without considering some pixels inside that area in the computation of the dot product. This allows for a **faster growth of the receptive field** in deeper layers than with standard convolutions. The intuition behind is that it is easier to integrate global context into the convolution operation. The following example shows a dilated convolution with a dilation factor of 2 on both the vertical and horizontal axis of the image.
<center><img src="https://imgur.com/CVVof0p.gif" align="middle" hspace="20px" vspace="5px"></center>

Given an input of size $(N,C_{in},H_{in},W_{in})$ the output of the convolution operation has size $(N,C_{out},H_{out},W_{out})$ with :

$$H_{out} = \left\lfloor \frac{H_{in}+2 \times padding[0]-dilatation[0] \times (kernel\_size[0] - 1) - 1}{stride[0]} + 1\right\rfloor$$

$$W_{out} = \left\lfloor \frac{W_{in}+2 \times padding[1]-dilatation[1] \times (kernel\_size[1] - 1) - 1}{stride[1]} + 1\right\rfloor$$

### Building Blocks of a CNN
CNNs built for classification tasks typically make use the following types of layers:
  - **Convolutional Layers**: Layers implementing the actual convolution as explained above. Their outputs are feature maps which are then passed through an activation function in order to introduce non-linearities into the system. Convolutional layers can be seen as extracting features that are passed on deeper into the model thus enabling the model to learn higher-level features that maket he classification task easier.
  - **Pooling Layers**: Downsampling or pooling layers concentrate the information so that deeper layers focus more on abstract/high-level patterns. You can apply strided convolutions to apply downsampling. A decreased image size also speeds up the processing time in general because less convolutions are necessary on subsequent layers. Furthermore, pooling allows for some translation invariance on the input. A common choice is max-pooling, where only the maximum value occurring in a certain region is propagated to the output.
  - **Dense Layers**: A dense or fully-connected layer connects every node in the input to every node in the output. This is the type of layer you already used for the linear regression model in the previous tutorial. If the input dimension is large, the amount of learnable parameters introduced by using a dense layer can quickly explode. Hence, dense layers are usually added on deeper levels of the model, where the pooling operations have already reduced the dimensionality of the data. Typically, the dense layers are added last in a classification model, performing the actual classification on the features extracted by the convolutional layers.
  
As an example, here is the architecture overview of the VGG16 model ([source](https://www.safaribooksonline.com/library/view/machine-learning-with/9781786462961/21266fa5-9e3b-4f9e-b3c6-2ca27a8f8c12.xhtml)).

<center><img src="https://imgur.com/sc9tqx6.png" align="middle" hspace="20px" vspace="5px"></center>

## The MNIST Data Set
With this brief recap of convolutional architectures, we are now ready to tackle the problem of hand-written digit classification from images. Let's first have a look at the contents of the MNIST data set. To do so, let's import all the libraries we need for this tutorial and define some useful helper functions.
"""

import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import math
import os

# for the tensorboard summary writers
from torch.utils.tensorboard import SummaryWriter

# Clear the tensorboard logs of previous runs
# !rm -rf /tmp/pytorch/mnist_cnn/logs/tensorboard

def plot_images(images, cls_true, cls_pred=None):
    """Plot 9 MNIST sample images in a 3x3 sub-plot."""
    assert len(images) == len(cls_true) == 9

    # Convert list of torch.Tensor to list numpy.array and 
    # rearrange dimensions from (1,H,W) to (H,W,1)
    images = [img.numpy().transpose((1,2,0)) for img in images]
    
    # Create figure with 3x3 sub-plots.
    fig, axes = plt.subplots(3, 3)
    fig.subplots_adjust(hspace=0.3, wspace=0.3)

    for i,ax in enumerate(axes.flat):
      # Plot image.
      ax.imshow(images[i][:,:,0], cmap='binary')

      # Show true and predicted classes.
      if cls_pred is None:
          xlabel = "True: {0}".format(cls_true[i])
      else:
          xlabel = "True: {0}, Pred: {1}".format(cls_true[i], cls_pred[i])

      ax.set_xlabel(xlabel)

      # Remove ticks 
      ax.set_xticks([])
      ax.set_yticks([])

# Import the data
log_dir = "/tmp/pytorch/mnist_cnn/logs"

# Define image transfomations :
# - ToTensor : converts images to torch.Tensors
transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

# Load MNIST train and test data
mnist = torchvision.datasets.MNIST(root=log_dir, train=True, download=True, transform=transforms)
mnist_test = torchvision.datasets.MNIST(root=log_dir, train=False, download=True, transform=transforms)

# Split MNIST into train and validation sets
# We set the train set to have 55000 samples and the validation set to have 5000
# We fix the generator so that the split is deterministic
mnist_train, mnist_val = torch.utils.data.random_split(mnist, lengths=[55000, 5000], generator=torch.Generator().manual_seed(42))

# We know that MNIST images are 28 pixels in each dimension.
img_size = 28
# Images are stored in one-dimensional arrays of this length.
img_size_flat = img_size * img_size
# Tuple with height and width of images used to reshape arrays.
img_shape = (img_size, img_size)
# Images are gray-scale, so we only have one image channel
num_image_channels = 1
# Number of classes, one class for each of 10 digits.
num_classes = 10

# Print some stats
print("Size of:")
print("- Training-set:\t\t{}".format(len(mnist_train)))
print("- Test-set:\t\t{}".format(len(mnist_test)))
print("- Validation-set:\t{}".format(len(mnist_val)))

# Get some sample images from the test set.
images, cls_true = zip(*[mnist_test[i] for i in range(9)])

# Plot the images and labels using our helper-function above.
plot_images(images=images, cls_true=cls_true)

"""## Building the Model
Let's now have a look at the core of this tutorial, namely how to build the actual CNN that is trained to predict the hand-written number on 28-by-28 gray-scale images. 

Lets start by selecting the device on which we want to train our model.

"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   
print(f"Device : {device}")

"""We create our `DataLoaders`."""

batch_size = 128

# Create DataLoaders 
train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)

"""Lets define the modules we will be using for our model:


*   **FullyConnectedLayer** : a simple Fully Connected layer with optional activation.
*   **ConvolutionPoolLayer** : a 2D convolution layer with optional 2D max pooling and optional activation function.

----

"""

class FullyConnectedLayer(torch.nn.Module):
    """
    Simple fully connected layer with optional activation.
    """
    def __init__(self, input_dim, output_dim, layer_name, activation=None, bias=True):            
        """
        Constructor for our FullyConnected layer.
        :param input_dim: The input dimension of the layer.
        :param output_dim: The desired output size we want to map to.
        :param layer_name: A name for this layer.
        :param activation: Activation function used on the output of the dense layer.
        :param bias: If set to False, the layer will not learn an additive bias. Default: True.
        """
        super().__init__()
        self.fc = torch.nn.Linear(input_dim, output_dim)
        self.activation = activation
        self.name = layer_name

    def forward(self, x):
        """
        Compute the forward pass of the FullyConnected layer.
        :param x: The input tensor.
        :return: The output of this layer. 
        """
        x = self.fc(x)
        if self.activation:
            x = self.activation(x)
        return x

class ConvolutionPoolLayer(torch.nn.Module):
    """
        Convolutional layer with optional MaxPooling and optional activation.
    """
    def __init__(self, in_channels, filter_size, out_channels, layer_name, activation, bias=True, use_pooling=True):
        """
        Constructor for ConvolutionPool layer.
        :param in_channels: Number of channels of the input.
        :param filter_size: Width and height of the square filter (scalar).
        :param out_channels: How many feature maps to produce with this layer.
        :param layer_name: A name for this layer.
        :param activation: Activation function used on the output of the layer.
        :param bias: If set to False, the layer will not learn an additive bias. Default: True.
        :param use_pooling: Use 2x2 max-pooling if True. Default: True.
        """   
        super().__init__()
        # Convolution parameters
        self.stride = (1, 1)
        self.filter_size = filter_size
        # Convolution operation - we do the padding manually in order to get tensorflow 'same' padding
        self.conv = torch.nn.Conv2d(in_channels, out_channels, self.filter_size, stride=self.stride, padding=0, bias=bias)
        # Use pooling to down-sample the image resolution?
        # This is 2x2 max-pooling, which means that we consider 2x2 windows and select the largest value
        # in each window. Then we move 2 pixels to the next window.
        self.max_pool = torch.nn.MaxPool2d(2, 2) if use_pooling else None
        # This adds some non-linearity to the formula and allows us to learn more complicated functions.
        self.activation = activation

    def get_padding_amount(self, shape):
        """
        Computes the amount of padding so that the input size is equal to output size.
        PyTorch doesn't provide 'same' padding so we use the implementation from TensorFlow.
        """
        _, _, input_h, input_w = shape
        output_h = int(math.ceil(float(input_h) / float(self.stride[0])))
        output_w = int(math.ceil(float(input_w) / float(self.stride[1])))
         
        if input_h % self.stride[0] == 0:
            pad_along_height = max((self.filter_size - self.stride[0]), 0)
        else:
            pad_along_height = max(self.filter_size - (input_h % self.stride[0]), 0)
        if input_w % self.stride[1] == 0:
            pad_along_width = max((self.filter_size - self.stride[1]), 0)
        else:
            pad_along_width = max(self.filter_size - (input_w % self.stride[1]), 0)
            
        pad_top = pad_along_height // 2 # amount of zero padding on the top
        pad_bottom = pad_along_height - pad_top     # amount of zero padding on the bottom
        pad_left = pad_along_width // 2     # amount of zero padding on the left
        pad_right = pad_along_width - pad_left  # amount of zero padding on the right

        return pad_left, pad_right, pad_top, pad_bottom

    def forward(self, x):
        """
        Compute the forward pass of the ConvolutionPoolLayer layer.
        :param x: The input tensor.
        :return: The output of this layer. 
        """
        padding = self.get_padding_amount(x.shape)
        x = torch.nn.functional.pad(x, padding)  # [left, right, top, bot]
        x = self.conv(x)
        if self.max_pool:
            x = self.max_pool(x)
        if self.activation:
            x = self.activation(x)
        return x

"""With these layers we can define our MNIST classifier."""

class ConvNet(torch.nn.Module):
    """
    Simple convolutional neural network. 
    """
    def __init__(self, feature_map_sizes, filter_sizes, activation=torch.nn.ReLU()):
        """
        Constructor for ConvNet.
        :param feature_map_sizes: list of out_channels for the convolution layers.
        :param filter_sizes: list of filter_size for each convolution layers.
        :param activation: Activation function used in the network. Default: ReLU.
        """   
        super().__init__()
        # Flatten layer
        self.flatten = torch.nn.Flatten() 
        # Fully Connected
        self.fc = FullyConnectedLayer(1152, 10, 'dense_layer', activation=None)
        # Softmax
        self.softmax = torch.nn.Softmax(dim=1)
        # Convolutions
        in_channels = 1
        self.convolutions = torch.nn.ModuleList()
        for i, (out_channels, filter_size) in enumerate(zip(feature_map_sizes, filter_sizes)):
            self.convolutions.append(ConvolutionPoolLayer(in_channels, filter_size, out_channels, f'conv{i}_layer', activation))
            in_channels = out_channels

    def get_probabilities(self, x):
        """
        Returns a softmax of the forward pass.
        """
        logits = self.forward(x)
        probs = self.softmax(logits)
        return probs 

    def forward(self, x):
        """
        Compute the forward pass of the network.
        :param x: The input tensor.
        :return: The activated output of the network. 
        """
        for conv in self.convolutions:
            x = conv(x)
        x = self.flatten(x)
        x = self.fc(x)
        return x

# create instance of our model
feature_map_sizes = [32, 64, 128]
filter_sizes = [5, 5, 5]
model = ConvNet(feature_map_sizes, filter_sizes, activation=torch.nn.ReLU())
# put the model in the device memory
model = model.to(device)

"""We can also define how we want to initilize the weights of our model."""

def weights_init(m, init="Normal"):
    """
    Initialize weights by drawing from a Gaussian distribution or
    Xavier initializers. We can initialize our weights differently 
    depending on the layer type.
    """
    if isinstance(m, torch.nn.Conv2d):
        if init == "Normal":
            torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)
            torch.nn.init.normal_(m.bias.data, mean=0.0, std=0.1)
        else:
            torch.nn.init.xavier(m.weight.data)
            torch.nn.init.xavier(m.bias.data)
    if isinstance(m, torch.nn.Linear):
        if init == "Normal":
            torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)
            torch.nn.init.normal_(m.bias.data, mean=0.0, std=0.1)
        else:
            torch.nn.init.xavier(m.weight.data)
            torch.nn.init.xavier(m.bias.data)

model.apply(weights_init)

"""Note that we did not use an activation function on the output of the dense layer. Generally speaking, it is not always reasonable to use an activation function on the last layer, depending on the distribution of your outputs. For example, if you used a `torch.nn.Tanh` activation on your output layer, all outputs would come to lie between -1 and 1. While an activation function on the outputs is a trick to define the output range for free, it is inherently biased. In other words, the distribution of output values does not match with the shape of the activation function. Depending on your problem, this might or might not make sense. 

For our case, we want the outputs of the model to be probabilities, i.e., the model should tell us what is the probability that a certain image belongs to each of the 10 classes. For this, we can use the softmax activation function, defined as follows:

$$
\sigma(\mathbf{z})_j = \frac{e^{\mathbf{z}_j}}{\sum_{k=1}^K e^{\mathbf{z}_k}}
$$

where $\mathbf{z}$ is our $K$-dimensional output vector `output` and $K$ refers to the number of classes that we are trying to predict, i.e. $K=10$ in our case. The softmax function essentially squashes its input between 0 and 1 and makes sure that all $K$ values sum up to 1. In other words, it produces a valid probability distribution over the number of classes.

<center><img src="https://imgur.com/te7R6BW.png" align="middle" hspace="20px" vspace="5px"></center>

So, how come we did not supply `torch.nn.Softmax` to our dense layer above? The reason is that when we feed softmax-activated values to our loss function, we introduce numerical instabilities that destabilize the training. The loss function measures how good the prediction of our network is. Ideally, if we feed an image depicting a hand-written 3 to our model, we want it to assign a probability of 1 to the class 3 and probabilities of 0 to all other classes. Thus, for every image we have a target distribution $q$, which is just a one-hot encoding of its label, and an estimated probability distribution $p$ which is the output of the model. A one-hot encoding of a label is simply a vector of zeros that has exactly one entry showing 1 corresponding to the index of that label. For example, the one-hot encoding of label 3 looks like this:

$$
\left[0, 0, 0, 1, 0, 0, 0, 0, 0, 0 \right]^T \in \mathbb{R}^{10}
$$

The only thing left is now to find a measure of the distance between those two distributions, i.e. how closely $p$ resembles the one-hot encoding $q$. For this, we can use the cross-entropy:

$$
H(p, q) = H(p) + D_{KL}(p || q) = -\sum\limits_x p(x) \log q(x)
$$

where $H(p)$ is the entropy of $p$ and $D_{KL}$ is the Kullback-Leibler Divergence. You can see from this formula that if the predicted probability $p$ is exactly a one-hot encoding, $H(p, q)$ will be 0, because the entropy $H(p)$ of a one-hot vector is 0 and the KL divergence will also be 0 because $p$ exactly matches $q$.

In PyTorch `torch.nn.CrossEntropyLoss` combines `torch.nn.LogSoftmax()` and `torch.nn.NLLLoss()` in one single class. The input is expected to contain raw, unnormalized scores for all C class. This mitigates the training instabilities described above.
"""

# Loss: Cross-Entropy
cross_entropy_loss = torch.nn.CrossEntropyLoss()

"""Now that we defined the model and the loss, we must also choose the optimizer. Before doing so, it might be a good idea to check how many trainable parameters we've created with the model definition above. This is both a sanity check and also gives you an intuition about the capacity of the model."""

# count total number of parameters including non trainable
total_params_count = sum(p.numel() for p in model.parameters())
# count total trainable parameters
trainable_params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total number of trainable parameters: {total_params_count}")
print(f"Number of trainable parameters: {trainable_params_count}")

"""With this, it is time to define our optimizer."""

# Optimization operation: Adam 
learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

"""At this point we are almost ready to start the training. Lets prepare some summary writers so that we can monitor the progress in TensorBoard."""

# Tensorboard
training_writer = SummaryWriter(log_dir + "/tensorboard/train")
validation_writer = SummaryWriter(log_dir + "/tensorboard/valid")

# Keep track of the numbers of epochs executed so far
NUM_EPOCH = 0
NUM_STEPS = 0

def train_step():
    """
    Train model for 1 epoch.
    """
    global NUM_STEPS

    model.train()

    for i, (image, label) in enumerate(train_loader):
        image, label = image.to(device), label.to(device) # put the data on the selected execution device
        optimizer.zero_grad()   # zero the parameter gradients
        output = model(image)  # forward pass
        loss = cross_entropy_loss(output, label)    # compute loss
        loss.backward() # backward pass
        optimizer.step()    # perform update
        
        NUM_STEPS += 1
        # log summaries to training_writer
        training_writer.add_scalar('loss', loss.item(), NUM_STEPS)

        train_accuracy = (torch.argmax(output, dim=1) == label).float().sum() / len(label) #get the accuracy for the batch
        training_writer.add_scalar('accuracy', train_accuracy, NUM_STEPS)
    
    return loss, train_accuracy


def evaluate():
    """
    Evaluate model on validation data.
    """
    model.eval()
    total_loss, total_accuracy = 0., 0.
    
    with torch.no_grad():
        for i, (image, label) in enumerate(val_loader):
            image, label = image.to(device), label.to(device) # put the data on the selected execution device
            output = model(image)  # forward pass
            loss = cross_entropy_loss(output, label)    # compute loss

            total_loss += loss.item()
            total_accuracy += (torch.argmax(output, dim=1) == label).float().sum()
        
    total_loss /= len(val_loader)
    total_accuracy /= len(mnist_val)

    return total_loss, total_accuracy

def train(n_epochs):
    """
    Train and evaluate model.
    """
    # use the global NUM_STEPS, NUM_EPOCH variable
    global NUM_STEPS, NUM_EPOCH

    for epoch in range(n_epochs):
        
        # train model for one epoch
        train_loss, train_accuracy = train_step()
        
        for name, weight in model.named_parameters():
            # Attach a lot of summaries to training_writer for TensorBoard visualizations.
            training_writer.add_scalar(f'{name}.mean', torch.mean(weight), NUM_EPOCH)
            training_writer.add_scalar(f'{name}.std_dev', torch.std(weight), NUM_EPOCH)
            training_writer.add_scalar(f'{name}.max', torch.max(weight), NUM_EPOCH)
            training_writer.add_scalar(f'{name}.min', torch.min(weight), NUM_EPOCH)
            training_writer.add_histogram(f'{name}.weights', weight, NUM_EPOCH)
            training_writer.add_histogram(f'{name}.grad', weight.grad, NUM_EPOCH)

        # evaluate 
        val_loss, val_accuracy = evaluate()
        # log summaries to validation_writer
        validation_writer.add_scalar('loss', val_loss, NUM_STEPS)
        validation_writer.add_scalar('accuracy', val_accuracy, NUM_STEPS)

        print(f"[Epoch {NUM_EPOCH}] - Training : accuracy = {train_accuracy}, loss = {train_loss}", end=" ")
        print(f"Validation : accuracy = {val_accuracy}, loss = {val_loss}")

        NUM_EPOCH += 1

"""Now let's train this model for a couple of epochs. """

train(3)
training_writer.flush()
validation_writer.flush()

"""You will see training loss, distribution of weight & bias parameter values.

You can see that the accuracy on the validation set steadily increases. Sometimes it might be interesting to see some visualizations of the learned convolutional filter weights or the outputs of layer. Let's define some helper functions to do that.
"""

def plot_conv_weights(weights, input_channel=0):
    """Helper-function for plotting convolutional weights."""
    # Convert it to a numpy array
    w = weights.numpy()

    # Get the lowest and highest values for the weights.
    # This is used to correct the colour intensity across
    # the images so they can be compared with each other.
    w_min = np.min(w)
    w_max = np.max(w)

    # Number of filters used in the conv. layer.
    num_filters = w.shape[0]

    # Number of grids to plot.
    # Rounded-up, square-root of the number of filters.
    num_grids = math.ceil(math.sqrt(num_filters))
    
    # Create figure with a grid of sub-plots.
    fig, axes = plt.subplots(num_grids, num_grids)

    # Plot all the filter-weights.
    for i, ax in enumerate(axes.flat):
        # Only plot the valid filter-weights.
        if i<num_filters:
            # Get the weights for the i'th filter of the input channel.
            # See new_conv_layer() for details on the format
            # of this 4-dim tensor.
            img = w[i, input_channel,:,:]

            # Plot image.
            ax.imshow(img, vmin=w_min, vmax=w_max,
                      interpolation='nearest', cmap='seismic')
        
        # Remove ticks from the plot.
        ax.set_xticks([])
        ax.set_yticks([])
    
    # Ensure the plot is shown correctly with multiple plots
    # in a single Notebook cell.
    plt.show()
    
    
def plot_conv_layer(layer, image):
    """Helper-function for plotting the output of a convolutional layer."""
    # Assume layer is a PyTorch op that outputs a 4-dim tensor
    # which is the output of a convolutional layer,
    # e.g. layer_conv1 or layer_conv2.
    
    # Make sur the layer has size (1, 1, 32, 32)
    image = image.unsqueeze(0)

    with torch.no_grad():
        # Feed image to the layer
        values = layer(image)

    # Convert it to a numpy array
    values = values.numpy()
    
    # Number of filters used in the conv. layer.
    num_filters = layer.weight.shape[0]

    # Number of grids to plot.
    # Rounded-up, square-root of the number of filters.
    num_grids = math.ceil(math.sqrt(num_filters))
    
    # Create figure with a grid of sub-plots.
    fig, axes = plt.subplots(num_grids, num_grids)

    # Plot the output images of all the filters.
    for i, ax in enumerate(axes.flat):
        # Only plot the images for valid filters.
        if i<num_filters:
            # Get the output image of using the i'th filter.
            # See new_conv_layer() for details on the format
            # of this 4-dim tensor.
            img = values[0, i, :, :]
            # Plot image.
            ax.imshow(img, interpolation='nearest', cmap='binary')
        
        # Remove ticks from the plot.
        ax.set_xticks([])
        ax.set_yticks([])
    
    # Ensure the plot is shown correctly with multiple plots
    # in a single Notebook cell.
    plt.show()

# Get 1st convolution layer filter
layer1 = model.convolutions[0].conv.weight.data

# Make sur that it isn't on gpu
layer1 = layer1.to("cpu")

# Plot filter weights
plot_conv_weights(weights=layer1)

"""Because we trained the model only for a few epochs, the validation accuracy is not overwhelming. Let's train more."""

# train for 10 more epochs
train(10)

# flush writers
training_writer.flush()
validation_writer.flush()

# plot convolution filters
layer1 = model.convolutions[0].conv.weight.data
layer1 = layer1.to("cpu")
plot_conv_weights(weights=layer1)

"""Once you trained the model for a longer period of time, you should achieve a validation accuracy of above 95 %. Next, we can look at how the model performs on some examples taken from the test set."""

# Get some sample images from the test set.
test_images, test_cls_true = zip(*[mnist_test[i] for i in range(9)])

# Transform into list of images into tensor of shape (10, 1, 32, 32)
test_images = torch.stack(test_images)

# Put them in the device memory
test_images = test_images.to(device)

# Feed the images into the model and get the predictions probabilities 
model.eval()
with torch.no_grad():
    test_probabilities = model.get_probabilities(test_images)

# test_probabilities has shape [9, 10], find the class with the highest probability for each
test_cls_predicted = torch.argmax(test_probabilities, axis=-1)

# make sure test_cls_predicted is on cpu and it into a list
test_cls_predicted = test_cls_predicted.to("cpu").tolist()

# Put test_images back on cpu
test_images = test_images.to("cpu")

# then visualize
plot_images(test_images, test_cls_true, test_cls_predicted)

test_probabilities[-1]

"""Assuming the model was trained for long enough, you should now see that the model has performed quite well on these 9 images.

May be for a certain image, you are also interested in visualizing the output of a convolutional layer, i.e. its resulting feature maps. For the first image in the test data set, i.e. the one showing number 7 in the above plot, this would look like follows:
"""

# Get 1st convolution layer filter
layer1 = model.convolutions[0].conv

# Make sur that it isn't on gpu
layer1 = layer1.to("cpu")

# then visualize
plot_conv_layer(layer1, mnist_test[0][0])

"""That's it - we've successfully trained a convolutional neural network for classification of hand-written digits. Lastly, let's not forget to clean up after us."""

training_writer.close()
validation_writer.close()

"""## TensorBoard
The most important one being the `accuracy` measurement. In this plot we see that the accuracy on both the validation and training set behave very similarly, which is what we want. If you would observe that the validation accuracy is again going down after some time, while the training accuracy keeps improving, this would be a strong indication that the model is overfitting.

Another trend we see from this plot is that accuracy quickly improves in the beginning and then starts to saturate over time. This is usually the convergence behavior one wants to see. If you see different behavior, e.g., no improvement at all over time, or no saturation phase, you might want to think about your choices for the learning rate, batch size, type of optimizer, or - in the worst case - the model architecture entirely. This is why it is beneficial to make certain parameteres configurable through the command line, so that you can quickly try different values for different hyper-parameters.

"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /tmp/pytorch/mnist_cnn/logs/tensorboard/

"""## Concluding Remarks and Exercises
In this tutorial you learned how to train and evaluate a simple convolutional neural network in PyTorch for the task of hand-written digit classification and how to visualize certain statistics of the training process by using TensorBoard. You also learned which design choices are typically required to build a CNN (size of filters, size of filter maps, strides, pooling, etc.) and how you can plot results from intermediate layers in the architecture.

Although the model built in this tutorial is fairly simple, you should now be able to understand more complex, state-of-the art architectures, some of which shall be listed here for your reference:
  - [VGG](https://arxiv.org/abs/1409.1556)
  - [DenseNet](https://arxiv.org/abs/1608.06993)
  - [ResNet](https://arxiv.org/abs/1512.03385)
  
Lastly, we encourage you to play around with this notebook. As a source of inspiration, here are a few (optional) exercises that you can try to solve:
  1. Implement strided convolutions to replace max pooling. What is the effect on the performance?
  2. Visualize the cross-entropy loss in TensorBoard.
  3. Play around with various architectures and compare them. For example, try to
    1. Use larger filter sizes while keeping the amount of layers fixed.
    2. Use more layers while keeping the filter sizes fixed.
    3. Introduce layers whose outputs are not max-pooled.
  
  In these experiments, check what happens to the number of parameters, the speed of training, the convergence rate etc. and try to find a model that beats the simple CNN we trained in this tutorial.
"""