# -*- coding: utf-8 -*-
"""1a_xor_mlp_for_students_with_blanks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/117IIQPs6zigZM2xQg3MHJhN_pwahf4O6

# Build Your Own MLP - XOR
In this Jupyter notebook you will implement a simple neural network from scratch.
To understand what a NN is we will limit ourselves to a very simple task, namely that of solving the exclusive or (XOR) function. We release a second notebook for self-study where you can then use the acquired concepts (and most of the code) to attack a real perception problem - that of eye-gaze estimation.

In this course, we normally use Deep Learning library [TensorFlow](https://tensorflow.org) to build neural network powered applications. However, for learning purposes it is better to actually implement everything yourself in plain Python and NumPy. 

## Setup
First let's import the necessary libraries and let's configure essential building blocks such as a pseudo-random number generator:
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
from pylab import rcParams

# %matplotlib inline

rcParams['figure.figsize'] = 12, 6

RANDOM_SEED = 56

np.random.seed(RANDOM_SEED)

"""We have now imported and configured useful helper libraries such as [matplotlib](https://matplotlib.org/) for plotting and [NumPy](http://www.numpy.org/) - the workhorse of numerical and scientific programming. If you are not familiar with these you should follow up on the [various](https://docs.scipy.org/doc/numpy/user/quickstart.html) [tutorials](https://cs231n.github.io/python-numpy-tutorial/) available online.

## Overview
The XOR problem is a toy example to show that linear models can fail at seemingly simple tasks. Consider this (truth-) table:

| x1 | x2 | y |
|--------|--------|--------|
|   0    |   0    |    0   |    
|   0    |   1    |    1   |    
|   1    |   0    |    1   |
|   1    |   1    |    0   | 

The task of the model is to predict the `y` column given the two input columns `x1` and `x2`. In other words, we must output `1` *iff* (if and only if) either of the inputs is `1` but not otherwise. Obviously, we want to learn this mapping from data, i.e. the 4 data points we have in this table. We can think of this as a classification problem that we can visualize as follows (*green* represents `1` and *blue* represents `0`):
"""

X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([ [0],   [1],   [1],   [0]])

# Colors corresponding to class labels y.
colors = ['green' if y_ == 1 else 'blue' for y_ in y] 

fig = plt.figure()
fig.set_figwidth(6)
fig.set_figheight(6)
plt.scatter(X[:,0],X[:,1],s=200,c=colors)
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()

"""Plotting the problem makes it clear why linear models will fail to do so - there is simply no straight line in $\mathbb{R}^2$ that can separate the green from the blue points. The neural network that we will build in the following is a simple feed-forward network that looks like this:

<img src="https://i.imgur.com/xOSTCVe.png" align="middle" hspace="20px" vspace="5px">

In the input layer we feed the two values $x_1$ and $x_2$ which we summarize into an input vector $\mathbf{x} = [x_1, x_2]^T \in \{0, 1\}^2$. The hidden layer does some computation and the output layer produces the result $\hat{y} \in \mathbb{R}$. The associated *weights* with each layer - aka learnable parameters of the network - are matrices $\mathbf{W}_1 \in \mathbb{R}^{2 \times 3}$ and $\mathbf{W}_2 \in \mathbb{R}^{3 \times 1}$. We can also write this down in a more mathematical way:

$$\mathbf{a} = g\left(\mathbf{h}\right) = g\left( \mathbf{x}^\mathrm{T}\mathbf{W}_1 \right)$$
$$\hat{y} = \mathbf{a}\mathbf{W}_2$$

Here, the function $g$ is an activation function, which is not explicitly shown in the above image. The choice of the activation function is crucial - we will come back to this later.


## Building Blocks
With this preliminaries out of the way, we are ready to implement our neural network and training procedure. For this, we need to implement the following building blocks:

* An **activation function**, i.e. $g$
* The **forward pass**, i.e. computing $\hat{y}$.
* A **loss function**, i.e. a quantity that measures how far away $\hat{y}$ is from the true value $y$.
* The **backward pass**, i.e. computing the gradients w.r.t. the loss function
* An **optimizer**, i.e. an algorithm that updates the trainable weights of the network based on the gradients

## Activation Function
Let's start with an easy task. In the lecture, you have seen that in deep neural networks linear layers are commonly followed by a non-linear activation function. Without these neural networks would only be able to approximate affine functions and hence would be a lot less powerful. 
The perceptron (MLP) algorithm uses a step-function as activation function. However, for various reasons in DL one uses different types of activation functions. One reason is that the activation function should be differentiable and should have a derivative that leads to fast convergence.

In this tutorial we will be working with the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) as activation function. It has several appealing properties: it is bounded, it is fully differentiable and has a positive derivative at any point. Furthermore, the sigmoid function maps real-valued inputs to the (0,1) range. This is useful because it allows us to interpret its output as a probability value. The sigmoid function is defined by: 
$$\sigma(\mathbf{x})=\frac{1}{1+e^{-\mathbf{x}}}$$

In order to train our neural network we will also need it's derivative which is given by:
$$\frac{\partial \sigma(\mathbf{x})}{\partial \mathbf{x}}=\sigma(\mathbf{x})\odot(1-\sigma(\mathbf{x}))$$
where $\odot$ stands for element-wise multiplication.
"""

def sigmoid(x):
  """
  Computes the sigmoid function sigm(input) = 1/(1+exp(-input))
  """
  return 1/(1 + np.exp(-x))

def sigmoid_(y):
  """
  Computes the derivative of sigmoid funtion. sigmoid(y) * (1.0 - sigmoid(y)). 
  The way we implemented this requires that the input y is already sigmoided
  """
  return y*(1.0-y)

"""To better understand the sigmoid function and its derivative, let's plot it."""

x = np.linspace(-10., 10., num=100)
sig = sigmoid(x)
sig_prime = sigmoid_(sig)

plt.plot(x, sig, label="sigmoid")
plt.plot(x, sig_prime, label="sigmoid prime")
plt.xlabel("x")
plt.ylabel("y")
plt.legend(prop={'size' : 16})
plt.show()

"""If you need some background info on how we got here check out the full derivation of this [here](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/).

## Forward Pass
Next, we have to implement the "body" of the neural network, in other words, how we compute the output from the inputs. For a feed-forward network like ours, this is actually just a bunch of matrix multiplications followed by sigmoid activations.

As we are usually dealing with a lot of data samples, we don't feed them to the model individually, but organise several of them together into **batches**. We can think of this as stacking the input vectors row-wise into a matrix $\mathbf{X}$. In our case, we only have 4 samples, so $\mathbf{X}$ will have shape $4 \times 2$. This allows us to rewrite the computations as:

$$
\mathbf{\hat{y}} = \sigma\left(\mathbf{X} \mathbf{W}_1\right)\mathbf{W}_2
$$

Now, both $\mathbf{H} := \mathbf{X} \mathbf{W}_1$ and $\mathbf{A} := \sigma(\mathbf{H})$ have dimensionality $4 \times 3$ and each hidden unit is computed by taking a weighted average of the samples' input features. This matrix multiplication is essentially how a dense (aka linear, feed-forward) layer looks like. Let's write that in code.
"""

def dense(inputs, weights):
    """A simple dense layer."""
    return inputs @ weights

"""With this, we are now ready to define the forward pass of our network. To this end, we have to initialize the weights, so let's do that first."""

input_size = 2
hidden_size = 3
output_size = 1
N = 4

def initialize_weights():
    # weights for hidden layer, shape: 2x3
    w1 = np.random.uniform(size=(input_size, hidden_size))
    # weights for output layer, shape: 3x1
    w2 = np.random.uniform(size=(hidden_size, output_size))
    return w1, w2
    
w1, w2 = initialize_weights()

"""Now we use these weights to define the forward pass."""

def forward_pass(X):
    # Step 1: Calculate weighted average of inputs (output shape: 4x3)
    net_hidden = dense(X,w1)
    
    # Step 2: Calculate the result of the sigmoid activation function (shape: 4x3)
    act_hidden = sigmoid(net_hidden)
    
    # Step 3: Calculate output of neural network (output shape: 4x1)
    y_hat = dense(act_hidden, w2)
    
    return act_hidden, y_hat

"""## Loss Function
Next we need to measure how good or bad the output of the network is - in other words, we need to define a loss function. The choice of loss function is crucial for the overall learning success as its gradients provide the supervision signal that drives the optimization. For this toy example, we simply choose the mean squared error (MSE) between the value predicted by the model and the ground-truth label.

$$ \mathcal{L} = \frac{1}{N}\sum_{i=1}^N\frac{1}{2} \| \hat{y}_i - y_i \|^2_2 $$

Note that the factor of $\frac{1}{2}$ is chosen for convencience only - you will see why when we compute the gradient in the next section. $N$ denotes the number of samples, which is $4$ in our case and $i$ is used to point to an individual sample. Let's write that in code.
"""

def mse(y_hat, y):
    residual = y_hat - y
    error = (np.sum(residual ** 2))/2.0/N
    return residual, error

"""## Backpropagation
Now we are ready to turn to the backbone of neural network training: 
the computation of the gradients and how they are propagated through the network. 
Computing gradients analytically can be straight-forward, but evaluating them numerically in an efficient way on a computer might be less so. 
This is where backpropagation comes into play: It computes the gradients necessary to update the weights in the network and does so efficiently. 
As the computation starts from the top layer, it is sometimes also called the backward pass.

Thanks to TensorFlow and Co. you usually don't have to compute the gradients yourself. 
Nevertheless, if you want to make headway in terms of deep learning it is important to really understand backprop and how gradients can be derived. 
In the following, we will not implement the backprop algorithm, but compute the gradients and use them directly for gradient descent.
Later in the lecture, you will learn more about how backprop is implemented.

The final implementation of the backward pass is not more than 7 lines of code for our simple network.
However, it is not always easy to see how one arrives at these lines. In the following sections, 
we are going to explain all the details necessary to fully comprehend the implementation of the backward pass.
Doing so, we use some calculus rules that you should be familiar with, but which will be treated again in the lecture during the upcoming weeks. 
So, don't worry if you don't yet understand all the details. However, we do encourage you to work through this exercise already and certainly 
come back to it when the respective content was treated in the lecture.

To compute the gradients in a neural network, it mainly boils down to knowing the multi-variate chain rule, 
how to differentiate vector and matrix quantities and doing all this very carefully to avoid easy mistakes. 
In the following, we'll guide you through this step-by-step.


### Implementation
Now we are ready to transfer the math into code.
"""

def backward(X, y_hat, act_hidden):
    # Step 1: Calculate error
    residual, error = mse(y_hat, y)
    
    # Step 2: calculate gradient wrt w2
    N = X.shape[0]
    dL_dy = residual*1.0/N # shape (4, 1)
    dy_dw2 = act_hidden    # shape (4, 3)
    dL_dw2 = dL_dy.T @ dy_dw2 # shape (1, 3)
    
    # According to the math, `dL_dw2` is a row-vector, however, `w2` is a column-vector.
    # To prevent erroneous numpy broadcasting during the gradient update, we must make
    # sure that `dL_dw2` is also a column-vector.
    dL_dw2 = dL_dw2.T
    
    # Step 3: calculate gradient wrt w1
    da_dh = sigmoid_(act_hidden) # shape (4, 3)
    dL_dw1 = np.zeros_like(w1)

    # Note: using `residual[:, 0]` instead of just `residual` is important here, as otherwise
    # numpy broadcasting will make `s` a 4x4 matrix, which is wrong
    s = residual[:, 0].reshape(N,1) # shape (4,1)
    dL_da = s @ w2.T    # shape (4, 3)
    dL_dh = dL_da*da_dh # shape (4, 3) element-wise multiplication
    for i in range(w1.shape[0]):
        for j in range(w1.shape[1]):
            x_i = X[:,i].T      # shape (1, 4)
            dL_dw1[i, j] = x_i @ dL_dh[:,j]

    return dL_dw2, dL_dw1, error

"""This is a naive implementation of course - the double for loop to populate `dL_dw1` can slow things down drastically. 
It would be better if we could implement this using only matrix and vector operations, so let's try. 
The expression $w_{2,j} \cdot \sigma'(h_{k,j})$ is easy to vectorize - it is just an element-wise multiplication of the 
weight vector $\mathbf{W}_2$ with the hidden representation $\mathbf{h}_k$ of sample $k$. We can do this for all samples 
$N$ at the same time if we stack $\mathbf{W}_2$ row-wise into a $N \times 3$ matrix. Then we can multiply it element-wise
with the residual vector $(\mathbf{\hat{y}} - \mathbf{y}) \in \mathbb{R}^{N \times 1}$ by broadcasting it again to a $4 \times 3$ matrix. 
Finally, the sum over the number of samples is a simple matrix product with the input matrix $\mathbf{X}$. In code, this looks like this:"""

def backward_faster(X, y_hat, act_hidden):
    # Step 1: Calculate error
    residual, error = mse(y_hat, y)
    
    # Step 2: calculate gradient wrt w2
    N = X.shape[0]
    dL_dy = 1.0 / N * residual  # shape (4, 1)
    dy_dw2 = act_hidden  # shape (4, 3)
    dL_dw2 = np.matmul(dL_dy.T, dy_dw2)  # shape (1, 3)
    
    # According to the math, `dL_dw2` is a row-vector, however, `w2` is a column-vector.
    # To prevent erroneous numpy broadcasting during the gradient update, we must make
    # sure that `dL_dw2` is also a column-vector.
    dL_dw2 = dL_dw2.T
    
    # Step 3: calculate gradient wrt w1
    da_dh = sigmoid_(act_hidden)  # shape (4, 3)asting by numpy
    dL_dw1 = 1.0 / N * np.matmul(X.T, np.matmul(residual, w2.T)*da_dh)   # shape (2, 3)
    
    return dL_dw2, dL_dw1, error

"""We can remove the manual tiling by realising that we can achieve the same effect with another matrix muliplication. 
This further simplifies the calculation of the gradient to a one-liner:"""

def backward_pass(X, y_hat, act_hidden):
    # Step 1: Calculate error
    residual, error = mse(y_hat, y)
    
    # Step 2: calculate gradient wrt w2
    N = X.shape[0]
    dL_dy = 1.0 / N * residual  # shape (4, 1)
    dy_dw2 = act_hidden  # shape (4, 3)
    dL_dw2 = dL_dy.T @ dy_dw2  # shape (1, 3)
    
    # According to the math, `dL_dw2` is a row-vector, however, `w2` is a column-vector.
    # To prevent erroneous numpy broadcasting during the gradient update, we must make
    # sure that `dL_dw2` is also a column-vector.
    dL_dw2 = dL_dw2.T
    
    # Step 3: calculate gradient wrt w1
    da_dh = sigmoid_(act_hidden)  # shape (4, 3)asting by numpy
    dL_dw1 = 1.0 / N * X.T @ ((residual @ w2.T)*da_dh)   # shape (2, 3) 
    
    return dL_dw2, dL_dw1, error

"""As you can see, the final computations to get the gradients look pretty innocent. However, to arrive here, quite some math and implementation tricks were involved. It is easy to see, that this will quickly become more complicated as soon as we add more and different types of layers (convolutional, recurrent, etc.).

## Optimization
Finally we are ready to write the last part of the pipeline: the optimizer. Here, we update the weights of the network based on the gradients that we computed in the backward pass. For this tutorial, we use the simplest version of stochastic gradient descent (SGD), i.e. the gradient update for $\mathbf{W}$ given gradients $\nabla\mathcal{L}\left(\mathbf{W}\right)=\frac{\partial\mathcal{L}}{\partial\mathbf{W}}$ is done in the following way:

$$\mathbf{W}^\prime = \mathbf{W} - \lambda\cdot\nabla\mathcal{L}\left(\mathbf{W}\right)$$

where $\lambda$ is the learning rate and $\mathbf{W}^\prime$ are the updated weights. In other words, we just take a step into the direction of steepest descent, scaled by $\lambda$. During training, we loop over the entire training set several times. One iteration over the entire set is called an **epoch** and each epoch might further divide the data set into several **batches**. Because our data set is quite small, we have only one batch, so there is actually not so much stochasticity involved in the training loop, but you get the idea.
"""

n_epochs = 10000
learning_rate = 0.1
training_errors = []

# re-initialize the weights to be sure we start fresh
w1, w2 = initialize_weights()

for epoch in range(n_epochs):
 
    # Step 1: forward pass
    act_hidden, y_hat = forward_pass(X)
    
    # Step 2: backward pass
    # dw2, dw1, error = backward_faster(X, y_hat, act_hidden)
    # dw2, dw1, error = backward(X, y_hat, act_hidden)
    dw2, dw1, error = backward_pass(X, y_hat, act_hidden)
    
    # Step 3: apply gradients scaled by learning rate
    w2 = w2 - learning_rate * dw2
    w1 = w1 - learning_rate * dw1
    
    # Step 4: some book-keeping and print-out
    if epoch % 200 == 0:
        print('Epoch %d> Training error: %f' % (epoch, error))
    training_errors.append([epoch, error])

# Plot training error progression over time
training_errors = np.asarray(training_errors)
plt.plot(training_errors[:, 0], training_errors[:, 1])
plt.xlabel('Epochs')
plt.ylabel('Training Error')

"""If everything is correct the error should be going down.


## Predictions and Visualization
Now that we have a trained neural network let's try and see if it can actually correctly solve the XOR problem. Does the following look like what you would expect to see?
"""

X = np.array([[0,0], [0,1], [1,0], [1,1]])
y_hat = [np.round(forward_pass(x)[1]) for x in X]

# Colors corresponding to class predictions y_hat.
colors = ['green' if y_ == 1 else 'blue' for y_ in y_hat] 

fig = plt.figure()
fig.set_figwidth(6)
fig.set_figheight(6)
plt.scatter(X[:,0],X[:,1],s=200,c=colors)
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()

"""In the beginning we said that a linear model will not be able to solve the XOR problem. Hence, the decision boundary of our neural network must be non-linear. But how exactly does it look? Where does the network switch from labeling a point als "blue" to labeling it "green"? To visualize this we can sub-sample the space $[0, 1]^2$, get the predictions from the model for all these points and visualize them."""

resolution = 20
min_x, min_y = 0.0, 0.0
max_x, max_y = 1.0, 1.0
xv, yv = np.meshgrid(np.linspace(min_x, max_x, resolution), np.linspace(min_y, max_y, resolution))
X_extended = np.concatenate([xv[..., np.newaxis], yv[..., np.newaxis]], axis=-1)
X_extended = np.reshape(X_extended, [-1, 2])
y_hat = [np.round(forward_pass(x)[1]) for x in X_extended]

# Colors corresponding to class predictions y_hat.
colors = ['green' if y_ == 1 else 'blue' for y_ in y_hat] 

fig = plt.figure()
fig.set_figwidth(6)
fig.set_figheight(6)
plt.scatter(X_extended[:,0],X_extended[:,1],s=200,c=colors)
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()

"""## Questions and Additional Exercises
Hopefully your network should now be able to solve the XOR problem succesfully. Here are a couple of questions to think about:

<ol>
    <li><b>No Activation</b><br>
    In the beginning we mentioned that the XOR problem cannot be solved with a simple linear regression. Try removing the sigmoid function and see for yourself. 
    <br><i>Note: you will have to update the gradient computation, but you can just use the identity function as an activation function, whose gradient should be straight-forward to compute.</i></li>
    <li><b>Activation Functions</b><br>
    Implement a different activation function, such as <a href="https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning" target="_blank">ReLU</a> or the hyperbolic tangent <a href="https://en.wikipedia.org/wiki/Hyperbolic_functions" target="_blank">tanh</a> and see how their choice affects training.
    <br><i>Note: you will have to update the gradient computation.</i>
    </li>
  <li><b>Number of Parameters</b><br>
  How many trainable parameters does our network contain?
  </li>
  <li><b>Hyper-Parameter Tuning</b><br>
  Successfull training of the neural network often depends on the choice of hyper-parameters. In our case, these are the learning rate $\lambda$, the total capacity of the network, and the number of epochs (one might also count the activation function and some other design decisions as a hyper-parameter, but we won't do that for this task). Play around with these parameters, i.e. choose high and low values for each of them and see how it affects training. Which parameter has the biggest effect? 
  </li>
  <li><b>Dense Layer with Bias</b><br>
  In the lecture, dense layers had biases, i.e. $\mathbf{h} = \mathbf{X}\mathbf{W} + \mathbf{b}$. Change the <code>dense</code> function so that it can use biases. How does this affect the gradients?
  </li>
  <li><b>Decision Boundary</b><br>
  Play around with the visualization of the decision boundary, e.g. increase the resolution or visualize it for a larger space. Intuitively, what would be a "good" decision boundary? How does the choice of hyper-parameters affect the boundary?
  </li>
  <li><b>Additional Layer</b><br>
  Add an additional layer to the network. For this you have to update the forward pass, the gradient computation in the backward pass and the gradient update in the optimizer.
  </li>
  <li><b>Chain Rule</b></li>
  Verify that
  
  $$\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}$$
  
  is indeed the same as
  
  $$ \left(\frac{\partial z}{\partial \mathbf{x}}\right)_i = \left( (\nabla_{\mathbf{y}} z)^T \cdot \mathbf{J}_{\mathbf{x}}(\mathbf{y}) \right)_i$$
  
</ol>
"""