# -*- coding: utf-8 -*-
"""5_pytorch_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLJzlxIk6ELxAW-_OjUdpwzOiBB8B-mY

# PyTorch Tutorial - RNNs

In this tutorial, you will learn how to build a recurrent neural network (RNN) to model time series data. 
More specifically, we will implement a model that generates text by predicting one character at a time. 
This model is widely based on this well-known [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. 
It is highly recommended to read through the post, as it is a great read. Additionally, we borrow some code from an 
implementation of this post in TensorFlow, taken from [here](https://github.com/sherjilozair/char-rnn-tensorflow).

In the following, we assume familiarity with the PyTorch tutorial presented in previous tutorials, 
i.e., you should be aware of PyTorch's core concepts. Furthermore, we will not demonstrate any usage of TensorBoard here
- for this, refer to the tutorial on CNNs.

This tutorial consists of:
  1. Introduction to RNNs
  2. A Look at the Data
  3. Building the Model
  4. Generating Text
  5. Concluding Remarks and Exercises
  6. _[Appendix]_

## Introduction to RNNs
### Learning from Sequences
A recurrent neural network (RNN) is a specialized architecture designed to model time series and sequence data. 
A question often associated with sequences is: Given a number of time steps in the sequence, 
what is the most likely next outcome? In other words, we want the model to output the following probability
$$
p(\mathbf{x}_t | \mathbf{x}_0, \dotsc, \mathbf{x}_{t-1})
$$

where $t$ is the index over time. This is also the task we want to solve in this tutorial, namely, given a sequence of characters, what is the most likely next character?

To answer the above question, it is a good idea to keep some sort of memory as we walk along the sequence because previous observations in the past influence the desired outcome in the future. As an example, consider predicting the last character in the following sentence (indicated by the question mark):

<center><img src="https://i.imgur.com/TKfgH7f.png" align="middle" hspace="20px" vspace="5px"></center>

For us, it is obvious that we should end it with double quotes. However, the model must _remember_ that there was an opening quotation mark that hasn't been closed yet. It gets even more trickier in the following example:

<center><img src="https://i.imgur.com/iQPGehZ.png" align="middle" hspace="20px" vspace="5px"></center>

To complete this sentence, the model must not only realize that the missing word is a noun, but it must also remember that we were talking about Italy in the beginning of the sentence and that the capital of Italy is Rome. To achieve this, it is not enough that the model only knows what characters are. It must have some notion of more abstract concepts of the underlying language - it has to learn how characters are formed to create words and how words are structured to build sentences and so on. Learning to understand text on all these different levels is difficult and being able to capture long-term dependencies is essential for such a task.

### Vanilla RNNs
In neural networks we model time-dependencies by introducing connections that loop back to the same node (hence the name _recurrent_). The recurrent connection typically updates the internal state/memory of the cell. Such an architecture can be drawn as is shown in the following ([source](http://www.deeplearningbook.org/contents/rnn.html)).

<center><img src="https://i.imgur.com/Uo6PCfN.png" align="middle" hspace="20px" vspace="5px"></center>

On the left you see the compact version of the graph. The model takes as input $\mathbf{x}$ and processes it over time while the recurrent connection updates the internal state $\mathbf{h}$ located in the memory cell. On the right side you see the unfolded version of the same graph, where we basically discretize the time dimension and make every time step explicit. This "unfolding" or "unrolling" of an RNN is what we have to do in practice when training. This step makes the model a finite, computational graph that we can actually work with.

The model shown above is a bit too simplistic - it just processes the input over time, but does not produce an output. A more realistic RNN looks like this ([source](http://www.deeplearningbook.org/contents/rnn.html)):

<center><img src="https://i.imgur.com/nLKmj8v.png" align="middle" hspace="20px" vspace="5px"></center>

Let's have a look at that in more detail. $\mathbf{x}$ is the input to the model, $\mathbf{o}$ is the output, and $L$ is a loss function that measures how much the output deviates from the target $\mathbf{y}$. In our case, $\mathbf{x}$ is a sequence of characters and the model produces character $\mathbf{o}_t$ at every time step $t$. We compare it to the target character $\mathbf{y}_t$ which is equivalent to the next character $\mathbf{x}_{t+1}$ in our sequence. Note that instead of outputting one specific character, we rather produce a probability distribution over all characters in the vocabulary (more on this later). This is similar to what we did with the CNN for image classification where we produce a probability of belonging to a class, instead of making a hard assignment.

The real magic of RNNs happens in the recurrent cell, which we sometimes also call _memory cell_ because it tracks the memory, or hidden state, $\mathbf{h}$. The question is now, how do we update the state $\mathbf{h}$? In the vanilla formulation we model it as follows:

$$
\begin{align}
\mathbf{a}^{(t)} &= \mathbf{W} \cdot \mathbf{h}^{(t-1)} + \mathbf{U} \cdot \mathbf{x}^{(t)} + \mathbf{b}\\
\mathbf{h}^{(t)} &= \tanh(\mathbf{a}^{(t)})\\
\mathbf{o}^{(t)} &= \mathbf{V} \cdot \mathbf{h}^{(t)} + \mathbf{c}
\end{align}
$$

Here the matrices $\mathbf{W}$, $\mathbf{U}$, $\mathbf{V}$, and biases $\mathbf{b}$ and $\mathbf{c}$ represent the learnable parameters of this model. Importantly, those parameters are _shared_ between time steps, i.e. every time step gets exactly the same copy of weights to work with.

### LSTM Cells
Despite its seeming simplicity, the vanilla RNN is already a powerful model. The only problem is that it turned out to be quite difficult to train such an RNN in practice. The reason for this is known as the _vanishing or exploding gradients problem_, which has been introduced in the lecture. In short, when we optimize the weights of an RNN, we end up backpropagating gradients through time. Because of the chain rule, gradients that arrive at layer $t$ are the product of a bunch of gradients of the layers from $t+1$ to $\tau$ (assuming we unfold the RNN for $\tau$ time steps in total). Now if each gradient in this large product is small (or big), the multiplication will make the resulting gradient even smaller (or bigger). This is especially a problem for "early" layers and if $\mathbf{\tau}$ is large, i.e., if we want to capture long-term dependencies. If you would like to read more about this, you can find a great article [here](http://neuralnetworksanddeeplearning.com/chap5.html).

So, what can we do to alleviate the problem of unstable gradients in RNNs? One answer was proposed in the seminal work by [Hochreiter and Schmidhuber, 1997](http://www.bioinf.jku.at/publications/older/2604.pdf) where they introduced Long Short Term Memory (LSTM) cells. These cells were designed to remember information for long periods of time and thus have made training of RNNs considerably easier. The following shows a schematic overview of the inner workings of an LSTM cell ([source](https://codeburst.io/generating-text-using-an-lstm-network-no-libraries-2dff88a3968)): 

<center><img src="https://i.imgur.com/FtcD5eR.png" align="middle" hspace="20px" vspace="5px"></center>

If you would like to read more about LSTM cells, we highly recommend to read [this excellent post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) from colah's blog. In a nutshell, the most important differences between a vanilla and a LSTM cell are:
  - The LSTM cell has two hidden variables instead of just one, the hidden state $\mathbf{h}$ and the cell state $\mathbf{c}$. 
  - Updates to the cell state are carefully protected by three gating functions consisting of a sigmoid layer and an element-wise multiplication (denoted by $\otimes$ or $\circ$).
  - Notice that the cell state $\mathbf{c}_{t-1}$ can more or less easily flow through the cell (top line in the above diagram) and thus propagate the information further into the next time step.
  
LSTMs have made training of recurrent structures much easier and have thus become the de-facto standard in RNNs. Of course, there are more cell types available (which you can find out about for example in colah's blog), but LSTMs are usually a good initial choice for a recurrent architecture.

## A Look at the Data
Let's now turn to our actual problem of training a character-level language model. Following Andrej Karpathy's [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), we are using the Shakespeare dataset which is just a text file containing some of Shakespeare's work. Here is an excerpt:

<center><img src="https://i.imgur.com/wPFXbqO.png" align="middle" hspace="20px" vspace="5px"></center>

What we want to achieve with the model can be summarized in the following diagram ([source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)).

<center><img src="https://i.imgur.com/yYLPfEz.png" align="middle" hspace="20px" vspace="5px"></center>

For the sake of simplicity, this diagram assumes a limited vocabulary of only 4 characters `[h, e, l, o]`. The input to the model is the word "hello". On the bottom you can see the input at each time step and a one-hot encoding of each character shown in red. Similarly at the top you find the target characters for each input character and a predicted confidence score shown in blue units. For example, in the first time step the confidence attributed to the letter `e` is 2.2, while the confidence for `o` is 4.1. Ideally, the confidence for the bold numbers in the blue boxes should be high, while for the red numbers it should be low. In the middle part of the diagram, shown in green, are the hidden states of the recurrent cells. Through this layer and the associated hidden state vectors, the information is propagated to future time steps, so that hopefully in the last time step the letter "o" will have a high confidence score.

We are now going to implement this architecture, but with a larger vocabulary and more training data. To use the Shakespeare data set, we need to preprocess the data, i.e., tokenize it, extract the vocabulary, and create batches of a certain sequence length (depending on how many time steps $\tau$ we want to unfold the RNN for). To do this, we strongely inspire ourselves from [this implementation](https://github.com/sherjilozair/char-rnn-tensorflow) written in TensorFlow.
"""

import codecs
import os
import collections
import pickle
import numpy as np
import torch

# for the tensorboard summary writers
from torch.utils.tensorboard import SummaryWriter

# Clear the tensorboard logs of previous runs
# !rm -rf /tmp/pytorch/shakespeare_rnn/logs/train

# Select device on which we will train our model 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

"""Lets first implement the `TextDataset` class to read and pre-process our data. 
Notice that `TextDataset` inherites from `torch.utils.data.Dataset` which allows us to get an iterable on it using a `torch.utils.data.DataLoader`.
"""

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, data_dir, seq_length, device, encoding='utf-8'):
        self.data_dir = data_dir
        self.seq_length = seq_length
        self.encoding = encoding
        self.rng = np.random.RandomState(42)

        input_file = os.path.join(data_dir, "shakespeare.txt")
        vocab_file = os.path.join(data_dir, "vocab.pkl")
        tensor_file = os.path.join(data_dir, "data.npy")

        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):
            print("reading text file")
            self.preprocess(input_file, vocab_file, tensor_file)
        else:
            print("loading preprocessed files")
            self.load_preprocessed(vocab_file, tensor_file)

        if self.tensor.shape[0] // self.seq_length == 0:
            assert False, "Not enough data. Make seq_length small."

    def preprocess(self, input_file, vocab_file, tensor_file):
        with codecs.open(input_file, "r", encoding=self.encoding) as f:
            data = f.read()
        counter = collections.Counter(data)
        count_pairs = sorted(counter.items(), key=lambda x: -x[1])
        self.chars, _ = zip(*count_pairs)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        with open(vocab_file, 'wb') as f:
            pickle.dump(self.chars, f)
        self.tensor = np.array(list(map(self.vocab.get, data)))
        np.save(tensor_file, self.tensor)

    def load_preprocessed(self, vocab_file, tensor_file):
        with open(vocab_file, 'rb') as f:
            self.chars = pickle.load(f)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        self.tensor = np.load(tensor_file)

    def __getitem__(self, index):
        x = self.tensor[index*self.seq_length:(index+1)*self.seq_length]
        x = torch.tensor(x).long()
        y = self.tensor[index*self.seq_length+1:(index+1)*self.seq_length+1]
        y = torch.tensor(y).long()

        return x.to(device), y.to(device)

    def __len__(self):
        return len(self.tensor) // self.seq_length

data_dir = "./ex3/"
log_dir = "/tmp/pytorch/shakespeare_rnn/logs/"

# Download the data
# !if [ ! -f eye_data.h5 ]; then wget -nv https://ait.ethz.ch/projects/shakespeare.txt?raw=true -O shakespeare.txt; fi

# Load the data into a torch.utils.data.Dataset
seq_length = 50
dataset = TextDataset(data_dir, seq_length, device=device)

batch_size = 128
# Build data loader
# drop_last=True drops the last batch if it is incomplete i.e if it doesn't contain batch_size elements
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
print("loaded vocabulary with {} letters".format(dataset.vocab_size))

"""When dealing with sequences it is often the case that not all have the same length. Hence, we need to ask ourselves two questions:
  1. How do we handle sequences of different lengths in the same batch?
  2. Do we want to use different sequence lengths at inference time than at training time?
  
To answer the first question, we can simply pad all sequences in a batch with dummy values to the maximum length occurring in that batch. 
This can be done using `torch.nn.utils.rnn.pad_sequence`. However, this is far from ideal as we are now training our model with incorrect values. 
A better solution is to let PyTorch automatically adjust its batch size to the variable length sequences by packing them. 
This is done using `torch.nn.utils.rnn.pack_padded_sequence`. 

Note that in our example, we do not need to pad the data, because our data loader already ensures that we have sequences of equal length. 
However, you should generally be aware of that caveat.

Regarding the second question, this is generally not an issue in PyTorch as the RNN cells don't explicitely depend on the sequence length. 
You may therefore unroll the RNN as long as you please. 

<center><img src="https://i.stack.imgur.com/LPHAs.jpg" align="middle" hspace="20px" vspace="5px"></center>

<center><img src="https://i.stack.imgur.com/Kuhh0.jpg" align="middle" hspace="20px" vspace="5px"></center>

Let's now have a look at our data.
"""

# Visualize some data
it = data_loader.__iter__() 
b, t = next(it)
b, t = b.to("cpu"), t.to("cpu")

print('total of {} batches of shape: {}'.format(len(data_loader), b.shape))
print('content of batch 0, entry 0, time steps 0 to 20')
print('input : {}'.format(b[0, :20].numpy()))
print('target: {}'.format(t[0, :20].numpy()))

# Print characters instead of integers
# Invert the vocabulary (note that this works because the vocabulary is distinct)
vocab_inv = {v: k for k, v in dataset.vocab.items()}

print('input : {}'.format([vocab_inv[i] for i in b[0, :20].numpy()]))
print('target: {}'.format([vocab_inv[i] for i in t[0, :20].numpy()]))

"""Great - we now have a way of tokenizing text and organizing it into batches of a given size and sequence length. 
Next, we'll look into how to build the actual RNN.

## Building the Model
We start by building the core of our model, the RNN with LSTM cells.
"""

class RNN_LSTM(torch.nn.Module):
    """
    Simple RNN with LSTM cells and linear projection head.
    """
    def __init__(self, hidden_size, num_layers, vocab_size, device):
        """
        Constructor for RNN_LSTM.
        :param hidden_size: The number of units for each LSTM cell.
        :param num_layers: The number of LSTM cells we want to use.
        :param vocab_size: The size of the text vocabulary.
        :param device: The device we are using to run the model.
        """   
        super(RNN_LSTM, self).__init__()
        # LSTM module
        self.lstm = torch.nn.LSTM(vocab_size, hidden_size, num_layers)
        # Fully connected layer - The output tensor of the lstm has shape [seq_len, batch, hidden_size],
        # i.e. it contains the outputs of the last cell for every time step.
        # We want to map the output back to the "vocabulary space", so we add a linear layer.
        # Importantly, the linear layer should share its parameters across time steps.
        # This is the case in PyTorch where the linear layer is only applied to the last dimenesion.
        self.fc = torch.nn.Linear(hidden_size, vocab_size)
        # Softmax layer
        self.softmax = torch.nn.Softmax(dim=-1)
        # Embedding layer
        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)
        self.embedding.weight.data = torch.eye(vocab_size)
        # Variables 
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # Set initial cell and hidden states to zero
        self.reset_hidden_state(batch_size)

    def reset_hidden_state(self, batch_size):
        """
        Reset initial cell and hidden states to zero. The batch size might be different during training
        and inference. The parameter allows to modify it consequently. 
        :param batch_size: batch size used on the model.
        """
        self.h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)
        self.c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)
        
    def one_hot(self, x):
        """
        Create a one-hot encoding of the inputs.
        In PyTorch : torch.nn.Embedding. = one_hot + a linear layer.
        Here linear layer is the identity matrix.
        :param x: The input tensor of size [batch_size, seq_length].
        :return: One-hot encoding of the inputs, has shape [batch_size, seq_length, vocab_size].
        """ 
        return self.embedding(x)

    def get_probabilities(self, x):
        """
        Returns a softmax of the forward pass.
        :param x: The input tensor of size (batch, seq_len).
        :return: For each timestep a categorical distribution on the vocabulary, 
        has shape [seq_length, batch_size, vocab_size].
        """
        logits = self.forward(x)
        probs = self.softmax(logits)
        return probs

    def forward(self, x):
        """
        Compute the forward pass of the network.
        :param x: The input tensor of size (batch, seq_len).
        :return: The activated output of the network. 
        """
        x = self.one_hot(x) # (batch, seq_len, vocab_size)
        x = x.transpose(0, 1) # (seq_len, batch, vocab_size)
        x, (self.h_0, self.c_0) = self.lstm(x, (self.h_0, self.c_0)) #  (seq_len, batch, hidden_size)
        x = self.fc(x)  #  (seq_len, batch, vocab_size)
        return x

"""In the following we have to take care of the remaining tasks: build the model, add a loss function, 
define the optimizer and define the training loop."""

hidden_size = 256
num_layers = 2

# build the model
model = RNN_LSTM(hidden_size, num_layers, dataset.vocab_size, device)
model = model.to(device)

# We use the same loss function as we did for the CNN tutorial, i.e. Cross-Entropy.
# This time, we have to compute it for each time step.
cross_entropy_loss = torch.nn.CrossEntropyLoss()

# count total number of parameters including non trainable
total_params_count = sum(p.numel() for p in model.parameters())
# count total trainable parameters
trainable_params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total number of trainable parameters: {total_params_count}")
print(f"Number of trainable parameters: {trainable_params_count}")

# Optimization algorithm : Adam 
learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Tensorboard
writer = SummaryWriter(log_dir + "/train")

# Keep track of the numbers of epochs executed so far
NUM_EPOCH = 0

def train(n_epochs):
    """
    Train model for n_epochs.
    """
    # use the global NUM_EPOCH variable
    global NUM_EPOCH, batch_size
    
    model.train()
    for epoch in range(n_epochs):    
        total_loss, total_accuracy = 0., 0.

        for i, (x, y) in enumerate(data_loader):
            y = y.transpose(0, 1)   # reshape y to (seq_len, batch)
            optimizer.zero_grad()   # zero the parameter gradients   
            model.reset_hidden_state(batch_size)    # reset the hidden states as we are training on a new sequence
            y_hat = model(x)    # forward pass
            loss = 0 
            for t in range(seq_length):
                loss += cross_entropy_loss(y_hat[t], y[t])  # compute loss
            loss.backward() # backward pass
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # clip the gradient to prevent exploding gradient
            optimizer.step()    # perform update 
            total_loss += loss.item()
            total_accuracy += (torch.argmax(y_hat, dim=2) == y).float().sum()

        total_loss /= len(data_loader)*seq_length
        total_accuracy /= len(data_loader)*batch_size*seq_length

        # log loss and accuracy to writer
        writer.add_scalar('loss', total_loss, NUM_EPOCH)
        writer.add_scalar('accuracy', total_accuracy, NUM_EPOCH)
        
        print(f"[Epoch {NUM_EPOCH}] - Training : accuracy = {total_accuracy}, loss = {total_loss}")
        NUM_EPOCH += 1

"""Now let's train this model for a couple of steps. Make sure you selected the GPU under Runtime > Change runtime type > Hardware Accelerator. Otherwise training will be quite slow."""

train(100)
writer.flush()

"""The loss is consistently decreasing, so that looks promising. 
Let's now look at another feature from PyTorch: saving and loading models. 
During training, it is a good idea to regularly save the model that you trained up to this point. 
Doing this with PyTorch is pretty straight-forward."""

# Define save path 
save_path = os.path.join(log_dir, 'checkpoints', f'RNN_{NUM_EPOCH}.tar')

if not os.path.exists(os.path.join(log_dir, 'checkpoints')):
    os.makedirs(os.path.join(log_dir, 'checkpoints'))

# Optional : save model on CPU 
model = model.to("cpu")

# Save the model, the optimizer state and current number of epochs
torch.save({
            'epoch': NUM_EPOCH,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }, save_path)

"""And that's it! Of course, saving a model is only useful if we can load it again (e.g. to do inference with it or to continue training). This is also quite easy to do. We just call a `torch.load` function."""

# Now restore model, optimizer state and number of epochs 
checkpoint = torch.load(save_path, map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
NUM_EPOCH = checkpoint['epoch']

# Load back model on desired device 
model = model.to(device)

"""Again, pretty easy. Note however that `load_state_dict` only loads the saved weights into the graph/optimizer, 
i.e. it assumes a suitable graph/optimizer exists already. If it doesn't, `load_state_dict` will most likely fail.

## Tensorboard

Lets inspect the accuracy and loss plots using TensorBoard.
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /tmp/pytorch/shakespeare_rnn/logs/train

"""## Generating Text
We have seen how we can train a model to predict a single character given an input sequence. 
But how can we use this model to generate text? This is what we will discuss in the following.

One way to do this is to generate text character-by-character and feeding the output of each time step back as input to the model. 
In other words, we get the output character for a given sequence, append that character to the sequence and repeat the whole process. 
This is illustrated in the following where the black text is the input sequence and the blue character is the output character.

<center><img src="https://i.imgur.com/NnToWQe.png" align="middle" hspace="20px" vspace="5px"></center>

Let's implement this for our model.
"""

def sample(prime_text, num_steps):
    """
    Sample `num_steps` characters from the model and initialize it with `prime_text`.
    :param : A string that we want to initialize the RNN with.
    :num_prime_textsteps: Integer specifying how many characters we want to predict after `prime_text`.
    :return: `prime_text` plus prediction.
    """
    # First we need to look up the initial text in the vocabulary and convert it a torch.Tensor.
    # We also need to adjust its size and place it on our device.
    input_prime = [dataset.vocab[c] for c in prime_text]
    input_prime = torch.tensor(input_prime).unsqueeze(0).to(device)

    # Feed the prime sequence into the model. 
    with torch.no_grad():
        out_probs = model.get_probabilities(input_prime) 
    
    # Now we have initialized the RNN with the given prime text. Let's see what it predicts
    # as the next character after the last from `prime_text`.
    # `out_probs` has shape `[1, len(prime_text), vocab_size]`
    next_char_probs = out_probs[0, -1].to("cpu")
    
    # `next_char_probs` is a probability distribution over all characters in the vocabulary.
    # How do we determine which character is next? We could just take the one that is most
    # probable of course. But let's implement something a bit different: actually sample
    # from the probability distribution.
    def weighted_pick(p_dist):
        cs = torch.cumsum(p_dist, dim=-1)
        idx = int(torch.sum(cs < torch.rand(1)).item())
        return idx

    next_char = weighted_pick(next_char_probs)
    
    # Save all predicted chars in a string
    predicted_text = vocab_inv[next_char]
    
    # Now we can sample for `num_steps`
    for _ in range(num_steps):

        # Convert next_char to a torch.Tensor, adjust size and feed to model.
        next_char = torch.tensor(next_char).unsqueeze(0).unsqueeze(0).to(device)
        with torch.no_grad():
            out_probs = model.get_probabilities(next_char)

        next_char_probs = out_probs[0, -1].to("cpu")
        
        # Sample from the distribution
        next_char = weighted_pick(next_char_probs)
        
        # Append to already predicted text
        predicted_text += vocab_inv[next_char]   
    
    return prime_text + predicted_text

# Reset hidden state before sampling
model.reset_hidden_state(batch_size=1)

# Put the model in eval mode
model.eval()

print(sample('The ', 500))

"""Depending for how long you trained the model, you should now be able to see some nice outputs. As an example, here is the output of a model that was trained for 5000 steps.

<center><img src="https://i.imgur.com/JhMzdEv.png" align="middle" hspace="20px" vspace="5px"></center>

We make a few observations:
  - The model successfully learned to create English words! Even if some might be purely imagined, they do sound at least like English words.
  - It also learned a great deal about the structure of the input data: mostly nice use of punctuation, it produces paragraphs that start with names in capital letters, etc.
  
Given the simplicity of our model, this is quite a nice result! Refer the Andrej Karpathy's [article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) to see some more results of the same model (applied to different datasets) and more visualizations of what's going on inside the RNN.
"""

# Cleanup
writer.close()

"""## Concluding Remarks and Exercises
In this tutorial you learned how to build an RNN that predicts the next character given a sequence of characters and how you can turn it into a generative model that produces sample text of arbitrary length. You should now be aware of the most important implementation details needed to train an RNN in PyTorch: potential need for padding, sharing weights when mapping RNN hidden states back to the output space, using initial and final state of the RNN to control the generation of sequences, etc.

In our example, we used an RNN to predict an output at each time step of the sequence. RNNs are however much more versatile than this and can be used in many more scenarios as shown here. ([source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))

<center><img src="https://i.imgur.com/6L3Pbdc.png" align="middle" hspace="20px" vspace="5px"></center>

PyTorch provides [functions](https://pytorch.org/docs/stable/nn.html#recurrent-layers) to implement such models, which are sometimes also called _sequence-to-sequence_ (seq2seq) models.

To gain a deeper understanding of RNNs, we encourage you to make a copy of this notebook and play around with it. Specifically, in the following are a couple of (optional) exercises that you might want to look at. Furthermore, you also find information about some more advanced topics in the appendix section, which we provide for the purpose of self-study.

  1. Read [Andrej Karpathy's](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [colah's](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blog posts.
  2. We did not use a validation set in this tutorial. Implement this and evaluate the validation set after every so-many epochs (like we did for the CNN tutorial).
  3. Compute the training accuracy, print it to the console and visualize it in TensorBoard. Check the [CNN notebook](https://colab.research.google.com/drive/1bx2dlJYutNitK-hlhp98OHO-5WZnrNyV) to see how you can use Tensorboard on Colab.
  4. Try out different cell types other than LSTMs, e.g. GRU.
  5. How could you add some regularization to this model? 
  6. Play around with the hyper-parameters. What happens if you omit the gradient clipping? How susceptible is the training to changes in the learning rate? Can you find a model that has less parameters but performs equally well?

## Appendix
### Writing Your Own RNN Cell
Sometimes the standard LSTM cell provided by PyTorch is just not enough, or sometimes you would like to do fancier stuff within a cell. Knowing how to write your own RNN cell can be very useful. E.g., we usually want to add a decoder on the outputs of the RNN, i.e. a dense layer that maps back to the output space. We can directly modify the RNN gates by implementing a new cell, and incorporte new features, such as adding a linear projection, by implementing a new recurrent layer based on this cell.

Code to adapte more complex LSTMs can be found [here](https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py).
"""

class CustomRNNCell(torch.nn.Module):
    """
    Implementation of a custom RNN cell. 
    Here this is just an LSTM cell but you may change the operations as you wish.
    """
    def __init__(self, input_size, hidden_size):
        super(CustomRNNCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.weight_ih = torch.nn.Parameter(torch.randn(4 * hidden_size, input_size))
        self.weight_hh = torch.nn.Parameter(torch.randn(4 * hidden_size, hidden_size))
        self.bias_ih = torch.nn.Parameter(torch.randn(4 * hidden_size))
        self.bias_hh = torch.nn.Parameter(torch.randn(4 * hidden_size))

    def forward(self, input, state):
        """
        :param: input of shape (batch, input_size): tensor containing input features
        :param: state = (h_0, c_0)
            h_0 of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch.
            c_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch.
        :return:
            h_1 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch
            c_1 of shape (batch, hidden_size): tensor containing the next cell state for each element in the batch
        """
        hx, cx = state
        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +
                 torch.mm(hx, self.weight_hh.t()) + self.bias_hh)
        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)

        ingate = torch.sigmoid(ingate)
        forgetgate = torch.sigmoid(forgetgate)
        cellgate = torch.tanh(cellgate)
        outgate = torch.sigmoid(outgate)

        cy = (forgetgate * cx) + (ingate * cellgate)
        hy = outgate * torch.tanh(cy)

        return hy, (hy, cy)

class DenseDecoderLayer(torch.nn.Module):
    """
    Custom recurrent layer based on our CustomRNNCell with a linear projection head.
    """
    def __init__(self, n_cells, input_size, hidden_size, output_size):
        super(DenseDecoderLayer, self).__init__()
        # LSTM part
        self.cells = torch.nn.ModuleList([CustomRNNCell(input_size if not i else hidden_size, hidden_size) for i in range(n_cells)])
        # Dnse layer to project back to 'vocabulary space'
        self.fc = torch.nn.Linear(hidden_size, output_size)

    def state_size(self):
        # just return the state size of the cell we are wrapping
        return self.cell.hidden_size
    
    def output_size(self):
        # must return the dimensionality of the tensor returned by this cell
        return self.output_size

    def forward(self, input, states):
        """
        input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.
        h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1. If proj_size > 0 was specified, the shape has to be (num_layers * num_directions, batch, proj_size).
        c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.
        """
        output_states = []
        # Removes temporal dimension and returns a tuple of all slices along that dimension.
        inputs = input.unbind(0) 
        # Iterate on all RNN cells
        for i, rnn_cell in enumerate(self.cells): 
            state = states[i]
            outputs = []
            # Unfold rnn_cell in time
            for j in range(len(inputs)):
                output, state = rnn_cell(inputs[j], state) # hy, (hy, cy)
                outputs += [output]
            output_states += [state] # (hT, cT)
            inputs = outputs
        outputs = torch.stack(outputs)

        # projection part
        out = self.fc(outputs)
        
        return out

"""This way, we can simplify our RNN_LSTM a bit using the customized RNN layer that we implemented. """

class RNN_LSTM(torch.nn.Module):
    """
    Simple RNN with LSTM cells.
    """
    def __init__(self, hidden_size, num_layers, vocab_size, device):
        """
        Constructor for RNN_LSTM.
        :param hidden_size: The number of units for each LSTM cell.
        :param num_layers: The number of LSTM cells we want to use.
        :param vocab_size: The size of the text vocabulary.
        :param device: The device we are using to run the model.
        """   
        super(RNN_LSTM, self).__init__()
        # LSTM + projection module
        self.custom_lstm = DenseDecoderLayer(num_layers, vocab_size, hidden_size, vocab_size)
        # Softmax layer
        self.softmax = torch.nn.Softmax(dim=-1)
        # Embedding layer
        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)
        self.embedding.weight.data = torch.eye(vocab_size)
        # Variables 
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # Set initial cell and hidden states to zero
        self.reset_hidden_state(batch_size)

    def reset_hidden_state(self, batch_size):
        """
        Reset initial cell and hidden states to zero. The batch size might be different during training
        and during inference. The parameter allows to modify it consequently. 
        :param batch_size: batch size used on the model.
        """
        self.h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)
        self.c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)
        
    def one_hot(self, x):
        """
        Create a one-hot encoding of the inputs.
        In PyTorch : one_hot + a linear layer = nn.Embedding.
        Here linear layer is the identity matrix.
        :param x: The input tensor of size [batch_size, seq_length].
        :return: One-hot encoding of the inputs, has shape [batch_size, seq_length, vocab_size].
        """ 
        return self.embedding(x)

    def get_probabilities(self, x):
        """
        Returns a softmax of the forward pass.
        :param x: The input tensor of size (batch, seq_len).
        :return: For each timestep a categorical distribution on the vocabulary, 
        has shape [seq_length, batch_size, vocab_size].
        """
        logits = self.forward(x)
        probs = self.softmax(logits)
        return probs

    def forward(self, x):
        """
        Compute the forward pass of the network.
        :param x: The input tensor of size (batch, seq_len).
        :return: The activated output of the network. 
        """
        x = self.one_hot(x) # (batch, seq_len, vocab_size)
        x = x.transpose(0, 1) # (seq_len, batch, vocab_size)
        x = self.custom_lstm(x, (self.h_0, self.c_0)) #  (seq_len, batch, hidden_size)
        return x

"""### Sharing Weights
Sharing weights between different structures of the graph is a very useful feature. In PyTorch it is perfectly safe to reuse the same parameter many times when defining a computational graph. While specifying the operations of the forward pass in the `forward` method, simply reuse the `torch.nn.Parameters` weights that you need. 

To showcase the power of PyTorch dynamic graphs, we will implement a very strange model: a third-fifth order polynomial that on each forward pass chooses a random number between 3 and 5 and uses that many orders, reusing the same weights multiple times to compute the fourth and fifth order.

This demonstration was taking from [here](https://pytorch.org/tutorials/beginner/examples_nn/dynamic_net.html).
"""

import random
import math

class DynamicNet(torch.nn.Module):
    def __init__(self):
        """
        In the constructor we instantiate five parameters and assign them as members.
        """
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))
        self.e = torch.nn.Parameter(torch.randn(()))

    def forward(self, x):
        """
        For the forward pass of the model, we randomly choose either 4, 5
        and reuse the e parameter to compute the contribution of these orders.

        Since each forward pass builds a dynamic computation graph, we can use normal
        Python control-flow operators like loops or conditional statements when
        defining the forward pass of the model.

        Here we also see that it is perfectly safe to reuse the same parameter many
        times when defining a computational graph.
        """
        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3
        for exp in range(4, random.randint(4, 6)):
            y = y + self.e * x ** exp
        return y

    def string(self):
        """
        Just like any class in Python, you can also define custom method on PyTorch modules
        """
        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'


# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = torch.sin(x)

# Construct our model by instantiating the class defined above
model = DynamicNet()

# Construct our loss function and an Optimizer. Training this strange model with
# vanilla stochastic gradient descent is tough, so we use momentum
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)
for t in range(30000):
    # Forward pass: Compute predicted y by passing x to the model
    y_pred = model(x)

    # Compute and print loss
    loss = criterion(y_pred, y)
    if t % 2000 == 1999:
        print(t, loss.item())

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f'Result: {model.string()}')

"""### Bi-directional RNNs
Bi-directional RNNs (BiRNN) are a powerful variant of recurrent networks. Instead of having only a hidden layer that connects states forward in time, BiRNNs have an additional layer that connects states backward in time. This means, that every time step can draw information from the past as well as the future to produce its prediction. The computational graph of a BiRNN looks something like this ([source](http://www.deeplearningbook.org/contents/rnn.html)):

<center><img src="https://i.imgur.com/Z1FggLN.png" align="middle" hspace="20px" vspace="5px"></center>

You can see that both recurrent layers are connected to the output, but they are not connected amongst themselves. For the task of building a character-level language model, we could technically use a BiRNN to predict single characters in gaps (or even more characters). However, it is then not straight-forward to turn this BiRNN into a generative model, because in order to generate predictions, we always need data from the future, too. Hence, the BiRNN - while powerful - is not always suitable for the problem at hand. PyTorch's API supports the creation of BiRNNs, and it is not much different then creating a uni-directional RNN, it suffices to set `bidirectional=True` in the recurrent layer constructor.
"""